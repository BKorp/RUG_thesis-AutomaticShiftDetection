# 1. RUG thesis Automatic Shift Detection
Code for a thesis on the subject of automating the detection of Creative shifts in (Netflix subtitle) translations

## 1.1. Table of Contents
- [1. RUG thesis Automatic Shift Detection](#1-rug-thesis-automatic-shift-detection)
  - [1.1. Table of Contents](#11-table-of-contents)
  - [1.2. General](#12-general)
  - [1.3. The Research](#13-the-research)
  - [1.4. Code, Data and Analyses](#14-code-data-and-analyses)
    - [1.4.1. Requirements](#141-requirements)
    - [1.4.2. Setup Preparation](#142-setup-preparation)
      - [1.4.2.1. Starting from Scratch](#1421-starting-from-scratch)
      - [1.4.2.2. Starting where I left off](#1422-starting-where-i-left-off)
        - [1.4.2.2.1. Downloaded Data and Results](#14221-downloaded-data-and-results)
    - [1.4.3. Main Code and Research](#143-main-code-and-research)
      - [1.4.3.1. The notebooks](#1431-the-notebooks)
      - [1.4.3.2. The code](#1432-the-code)

## 1.2. General
This is the code that was used for a Master's Thesis in Information Science at the [University of Groningen]() in the Netherlands.

The focus of the research was to look into automating creativity for translation. Specifically, the following title of my thesis gives a good indication:

How Creative are Translated Subtitles? \
Automating the Detection of Creative Shifts in
English-to-Dutch Subtitles.

I made use of Netflix data which was compiled by a preceding Master's student, van der Heden, in 2021. This was also the starting point for my research.

## 1.3. The Research
To research the different aspects of creativity detection for translation, I built upon the work by [van der Heden (2022)](https://arts.studenttheses.ub.rug.nl/30430/) and [Jiahui Liang (2021)](https://arts.studenttheses.ub.rug.nl/29672/). Specifically, the use of cosine similarity (and its inverse, distance) taken from static embeddings, became the starting point of the research shown here.
It lead to the following three systems that were tested:
- Static embeddings, using [Gensim](https://radimrehurek.com/gensim/) taken from [FastText](https://fasttext.cc/) [(2016)](https://dblp.org/rec/journals/corr/JoulinGBM16.bib)
- Contextual embeddings, making use of the models created for [Sentence Transformers](https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2) [(2019)](http://arxiv.org/abs/1908.10084)
- Syntactical similarity, using the [ASTrED](https://github.com/BramVanroy/astred/) project [(2021a](https://doi.org/10.1007/978-3-030-69777-8_10),[b)](https://doi.org/10.3389/fpsyg.2021.681945)

Next to this, I also recreated and subsequently updated the preprocessing workflow to remove any potential lingering issues.

## 1.4. Code, Data and Analyses
To run the code, a couple of things have to be taken into consideration:
- The requirements for Python
- The setup preparation
  - Downloading the data for running what I have done without processing it again
  - Downloading the results for running what I have done without processing again
- Running the main code for
  - Preprocessing
    - XML to SRT
    - SRT to TXT
    - Cleanup
    - Tokenization
    - Sentence alignment
    - Word alignment
  - Analysis
    - Static
    - Context
    - Syntax
- Understanding the Jupyter Notebooks with analyses

### 1.4.1. Requirements
All code made use of `Python 3.10.11`. It is also recommended to use a system with `Bash`, as some aspects such as preprocessing and system setup make use of `Bash`. It is therefore recommended that you make use of `MacOS`, `Windows WSL`, or `Linux`. I have tested everything in `Linux`.

Firstly, the system here expects the use of a `venv-like` virtual environment.
It is recommended to make it here, or make a link to it here called 'env':
```bash
python -m venv env --upgrade-deps
```
Or alternatively, creating a symbolic link in Bash:
```bash
ln -s /path/to/venv_directory env
```

Subsequently, for the following install task, make sure to `activate` the venv:
```bash
source ./env/bin/activate
```
Afteating the venv, all requirements can be installed using `requirements.txt` in the `root` directory:
```bash
pip install -r requirements.txt
```

### 1.4.2. Setup Preparation
With the requirements installed, there are two options:

#### 1.4.2.1. Starting from Scratch
Run the setup.sh file in the root folder:

This will do the following (if this is read months or years later, and downloads have stopped working, contact me or my supervisors, see contact up above.):
- Activate the data_prep.py in data to download and prepare all data that has been generated by me, if possible
- Go into external_tools to prepare and download all external tools that are required to run the program.

This is a basic setup that sets the project up for starting from scratch with the original Heden data and folders for reproduction and new processing.


#### 1.4.2.2. Starting where I left off
The overall data can be found at:

[`Google Drive`](https://drive.google.com/drive/folders/1nB7rlrbzzi_UQoE8o4Pf_LUI3hbYNcf5?usp=sharing)

To set up the project, simply:
- Download `data.zip` and `results.zip`
- Remove the `data` and `results` folders from the root folder
- Unzip the downloaded `data.zip` and `results.zip` into the root directory
- Make sure you now have a `data` folder and a `results` folder
- If you want to have all external tools, go into external tools:
  ```bash
  cd external_tools
  bash tools.sh
  ```
- If you want to make use of `static embeddings and static inference`, you will also need the contents of the `embeddings` folder, this is not included by default because the file size of all embeddings is large (around `40 gb uncompressed`)
  - I recommend you `download `each` file individually, to make sure Google Drive does not create one large file.
  - The files should be `unzipped` into `./data/_embeddings/fasttext/`

The data and results folders are cluttered with main and intermediate files, it is therefore recommended to make use of the `notebook files` in `src`.

##### 1.4.2.2.1. Downloaded Data and Results
In the situation where you want to retrieve files from the `downloaded` `data` and `results` folders, these are the aspects to look out for:

- The `data folder` contains the folders for `the preceding thesis` using static embeddings and `the new system`
  - It contains the original xml files and all files that are a part of `pre-processing`
- For the `gold data`, all `early gold data` can be found in the `root of the data folder` and `the new run subfolder`
- The `final iterations of all important output files` can be found `in results`, which have been used for the system inference in the research notebooks
  - The `results` folder also contains the `generated figures` in pdf format


### 1.4.3. Main Code and Research
Within `src` there are `three folders` and `five notebooks`.

#### 1.4.3.1. The notebooks
The notebooks are named chronologically for the order of reading.
They are full of comments as the code itself is, but should instead be seen as an addition (a source, to be precise) for the Master thesis.

The notebooks are ordered internally through the use of titles. Each notebook has a slightly different ordering, which should become clear when reading them.

In some cases, if rerunning the notebooks, it may be necessary to take the following into account:
- Make sure to run import blocks to `make sure all imports are loaded`
- In some cases you can start at a certain section instead of running the entire notebook, this is usually possible if there is a separate `import` block
- `Some sections use the same variable names`, so make sure you do not run blocks from multiple different sections directly after each other without making sure you do not need that information anymore.

#### 1.4.3.2. The code
Most of the code can be subdivided into three different types:
- helper code
- preprocessing
- detection

**Helpers** \
Helper code can be found outside of `src`, and in `helpers` folders. This usually contains things like folder prep, timestamping, data_formatting etc.

**Preprocessor** \
The preprocessing code can be found in the `src/preprocessor` folder

```bash
cd src/preprocessor
python preprocessor.py
```

By running `preprocessor.py`, all data (if prepared) will be converted from xml, to many different data types:
- srt (translation format, used as an intermediate format here)
- txt (main basic format, cleaned, but not tokenized or aligned)
- lfa (basic extension meant for sentence aligned data)
- lfa_i (indices from Vecalign per line, which are used for sentence alignment)
- lfa_t (tokenized and sentence aligned data)
- wa (word alignments for lfa_t files in Pharaoh format)

The workflows that exist for this system are:
- `new` - making use of an updated cleanup, a different tokenizer (stanza), a new sentence aligner (Vecalign), and a word alignment setup taken from (ASTrED) using Aswesomealign
- `reproc`* - Using Moses tokenizer, LF Aligner sentence alignment, and default word alignment Awesomalign.

*_This is currently disabled, as I decided against looking into this for my analyses and thesis beyond guiding my early setup and work. It can be reactivated with the caveat that it may not all work perfectly anymore._

**Detection** \
The Detection step contains code for all aspects, though it should be noted that the workflow is less straightforward than the preprocessor.

The `detector.py` script can be called with `--help` or `-h` for more information on its functioning:
```bash
cd src/detectors
python detector.py -h
```
But for bash inference, it is recommended to make use of `detector.sh`, which runs multiple* prepared runs of `detector.py`:
```bash
cd src/detectors
bash detector.sh
```

*_It should be noted that syntax does not follow this setup, and makes use of the data that is generated for contextual information. The setup for this data can be found in the syntactic notebook which expects that all other data has already been created._